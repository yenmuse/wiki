created: 20211101140316860
modified: 20211101141632603
tags: cache
title: cache相关概念

|!Term|!Definition|!Note|
|Line|A cache line is the smallest  block of data that the cache operates on. The cache line is typically much larger than the size of data access from the dsp or the next high level of memory. For instance, although the DSP may request single bytes from memory, on a read miss the cache reads an entire line's worth of data to satisfy the request.||
|Line Frame|A location in a cache the holds cached data (one line), an associated tag address, and status information for the line. The status information can include whether the line is valid, dirty, and the current state of that line's least-recently-used(LRU).||
|Line Size|The size of a single cache line, in bytes.||
|Tag|A storage element containing the most-significant bits of the address stored in a particular line. Tag addresses are stored in special tag memories that are not visible to the dsp. The cache queries the tag memories on each access to determine if the access is a hit or a miss.||
|Set|A collection of line frames in a cache that a single address can potentially reside. A direct-mapped cache contains one line frame per set and an N-way set-associative cache contains N line frames per set. A fully-associative cache has only one set that contains all of the line frames in the cache.||
|Set-associative|A set-associative cache contains multiple line frames that each lower-level memory location can be held in cache. When allocating room for a new line of data, the selection is made based on the allocation policy for the cache. The C64+/C66x devices employ a least-recently used allocation policy for its set-associative caches.[img[file:./resources/picbed/cache_associative.png]]||
|Capacity Miss|A cache miss that occurs because the cache does not have sufficient room to hold the entire working set of a program||
|Compulsory Miss|Sometimes referred to as a first-reference miss. A compulsory miss is a cache miss that must occur because the data has had no prior opportunity to be allocated in the cache. Typically a, compulsory misses for particular pieces of data occur on the first access of that data. However, some cases can be considered compulsory even if they are not the first reference to the data. Such cases include repeated write misses on the same location in a cache that does not write allocate and cache misses to non-cacheable locations.||
|Conflict Miss|A cache miss that occurs due to the limited associativity of a cache, rather than due to capacity constraints.||
|LRU|Least-Recently-Used. For set-associative and fully associative caches, the least-recently-used allocation refers to the (LRU) allocation method to choose among line frames in a set when allocating space in the cache. When all of the line frames in the set that the address maps to contain valid data, the line frame in the set that was read or written the least recently is selected to hold the newly cached data. The selected line frame is then evicted to make room for the new data.||
|Read Allocate|A read allocate cache only allocates space in the cache on a read miss. A write miss does not cause an allocation to occur unless the cache is also a write-allocate cache. For cache that does not write allocate, the write data would be passed on to the next lower-level cache.||
|Write Allocate|A write-allocate cache allocates spaces in the cache when  a write miss occurs. Space is allocated according to the cache's allocation policy(LRU, for example), and the data for the line is read into the cache from the next lower level of memory. Once the data is present in the cache, the write is processed. For a write back cache, only the current level of memory is updated, the write data is not immediately passed to the next level of memory.||
|Write back cache|A write back cache will only modify its own data on a write hit. It will not immediately send the update to the next lower-level of memory. The data will be written back at some future point, such as when the cache line is evicted or when the lower-level memory snoops the address from the high-level memory. It is also possible to directly initiated a write back for a range of addresses using cache control registers. A write hit to a write back cache causes the corresponding line to be marked as dirty-that is, the line contains updates that have yet to be sent to the lower levels of memory.||
|Write through cache|A write through cache passes all writes to the lower-level memory. It never contains updated data that has not passed on to the lower-level memory. As a result, cache lines can never be dirty in a write through cache. The C66x devices do not utilize write through caches.||
|Miss Pipelining|The process of servicing a single cache miss is pipelined over several cycles. By pipelining the miss, it is possible to overlap the processing of several misses, should many occur back-to-back. The net result is that much of the overhead for the subsequent misses is hidden, and the incremental stall penalty for the additional misses is much smaller than that for a single miss taken in isolation.||
|Memory coherent|Informally, a memory system is coherent if any read of a data item returns the most recently written value of that data item.||
|Memory consistency|Also referred as memory ordering. Defines what order the effects of memory operations are made visible in memory. Strong memory ordering at a given level in the memory hierarchy indicates it is not possible to observe the effects of memory accesses in that level of memory in an order different than program order. Relaxed memory ordering allows the memory hierarchy to make the effects of memory operations visible in a different order. Note that strong ordering does not require that the memory system execute memory operations in program order, only that it makes their effects visible to other requestors in an order consistent with program order. [img[file:./resources/picbed/cache_cohe_vs_cons.png]]||
|Thrash|An algorithm is said to be thrash the cache when its access pattern cause the performance of the cache suffer dramatically. Thrash can occur for multiple reasons. One possible situation is that the algorithm is accessing too much data or program code in a short time frame with little or no reuse. That is, its working set is too large and thus the algorithm is causing a significant number of capacity misses. Another situation is that the algorithm is repeatedly accessing a small group of different addresses that all map to the same set in the cache, thus causing an artificially high number of conflict misses.||